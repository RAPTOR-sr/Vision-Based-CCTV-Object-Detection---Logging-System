import torch
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import cv2
import numpy as np
import time # For adding timing info if desired

class CaptionGenerator:
    """
    Generates captions for image regions using the BLIP model.
    """
    def __init__(self, model_id="Salesforce/blip-image-captioning-base", device=None):
        """
        Initializes the BLIP captioning model and processor.

        Args:
            model_id (str): The Hugging Face model ID for BLIP.
                            "Salesforce/blip-image-captioning-base" is smaller/faster.
                            "Salesforce/blip-image-captioning-large" is more accurate but slower/larger.
            device (str, optional): The device to run the model on ('cuda', 'cpu').
                                    Defaults to 'cuda' if available, else 'cpu'.
        """
        
        print(f"Initializing BLIP model: {model_id}...")
        self.model_id = model_id
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = 'cpu'
        print(f"Using device: {self.device}")

        try:
            self.processor = BlipProcessor.from_pretrained(self.model_id)
            print("Processor loaded.")
            self.model = BlipForConditionalGeneration.from_pretrained(self.model_id)
            self.model.to(self.device)
            self.model.eval() # Set model to evaluation mode
            print(f"BLIP model '{self.model_id}' loaded successfully to {self.device}.")
        except Exception as e:
            print(f"Error loading BLIP model: {e}")
            print("Please ensure 'torch' and 'transformers' are installed correctly,")
            print("and you have an internet connection for the first download.")
            self.processor = None
            self.model = None

    def generate_caption(self, frame, detection):
        """
        Generates a caption for the detected object's ROI using BLIP.

        Args:
            frame (numpy.ndarray): The full frame (BGR) containing the object.
            detection (dict): The detection dictionary containing 'bbox' and 'class_name'.

        Returns:
            str: A descriptive caption generated by BLIP, or a fallback caption.
        """
        if self.model is None or self.processor is None:
            return f"a {detection['class_name']} (BLIP model not loaded)"

        class_name = detection['class_name']
        x1, y1, x2, y2 = detection['bbox']

        # Ensure coordinates are within frame boundaries
        h, w = frame.shape[:2]
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w - 1, x2), min(h - 1, y2)

        # Crop the object ROI
        if x1 >= x2 or y1 >= y2:
            print("Warning: Invalid bounding box, skipping caption generation.")
            return f"a {class_name} (invalid ROI)" # Invalid bbox

        roi_bgr = frame[y1:y2, x1:x2]

        if roi_bgr.size == 0:
            print("Warning: Empty ROI, skipping caption generation.")
            return f"a {class_name} (empty ROI)"

        try:
            # Convert ROI from BGR (OpenCV) to RGB (PIL)
            roi_rgb = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(roi_rgb)

            # Prepare image for BLIP
            # Using unconditional generation (no text prompt)
            inputs = self.processor(images=pil_image, return_tensors="pt").to(self.device)

            # Generate caption
            start_time = time.time()
            with torch.no_grad(): # Important for inference efficiency
                out = self.model.generate(**inputs, max_length=50, num_beams=4) # Adjust max_length/num_beams as needed
            end_time = time.time()

            # Decode caption
            caption = self.processor.decode(out[0], skip_special_tokens=True)

            # Basic cleanup (optional: remove generic prefixes if they often appear)
            # if caption.startswith("a photography of"):
            #    caption = caption.replace("a photography of", "", 1).strip()
            # elif caption.startswith("a picture of"):
            #      caption = caption.replace("a picture of", "", 1).strip()

            # print(f"BLIP caption for {class_name} (took {end_time - start_time:.2f}s): {caption}")
            return caption.strip() # Return the generated caption

        except Exception as e:
            print(f"Error during BLIP caption generation for {class_name}: {e}")
            # Fallback caption if BLIP fails
            return f"a {class_name} (captioning error)"